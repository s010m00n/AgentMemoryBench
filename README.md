<div align="center">
  <img src="figures/AgentMemoryBench.svg" width="100%" alt="Agent Memory Bench" />

  <br/>
  <br/>

  <a href="https://github.com/s010m00n/AgentMemoryBench/stargazers">
    <img src="https://img.shields.io/github/stars/s010m00n/AgentMemoryBench?style=for-the-badge&logo=github&color=ff6b6b" alt="Stars">
  </a>
  <a href="https://github.com/s010m00n/AgentMemoryBench/network/members">
    <img src="https://img.shields.io/github/forks/s010m00n/AgentMemoryBench?style=for-the-badge&logo=github&color=ee5a6f" alt="Forks">
  </a>
  <a href="https://github.com/s010m00n/AgentMemoryBench/issues">
    <img src="https://img.shields.io/github/issues/s010m00n/AgentMemoryBench?style=for-the-badge&logo=github&color=c44569" alt="Issues">
  </a>
  <a href="https://github.com/s010m00n/AgentMemoryBench/blob/main/LICENSE">
    <img src="https://img.shields.io/badge/License-MIT-brightgreen?style=for-the-badge" alt="License">
  </a>

  <br/>
  <br/>

  <p align="center">
    <strong>A Unified Benchmark for Continual Agent Memory</strong>
    <br />
    <br />
    A comprehensive benchmark for evaluating memory mechanisms in LLM-based agents across continual learning scenarios, supporting both <strong>system memory</strong> (task workflows) and <strong>personal memory</strong> (user preferences).
    <br />
    <br />
    <a href="#overview">Overview</a> â€¢
    <a href="#evaluation-modes">Evaluation Modes</a> â€¢
    <a href="#quick-start">Quick Start</a> â€¢
    <a href="#creating-custom-memory-mechanisms">Custom Memory</a> â€¢
    <a href="#implemented-memory-mechanisms">Methods</a>
  </p>
</div>

---

<h2 id="overview">ğŸ¯ Overview</h2>

AgentMemoryBench provides a unified framework to evaluate how LLM agents learn and retain two types of memory:
- **System Memory**: Task workflows and execution patterns
- **Personal Memory**: User preferences and dialogue context

The benchmark spans **6 interactive tasks** across 4 grounding types:
- **Code-grounded**: Database (SQL), Operating System (Shell), Knowledge Graph (SPARQL)
- **Embodied**: ALFWorld (household tasks)
- **Web-grounded**: WebShop (e-commerce)
- **Dialogue-grounded**: LoCoMo (long-term conversations)

<h2 id="evaluation-modes">ğŸ“Š Evaluation Modes</h2>

AgentMemoryBench supports **5 complementary evaluation modes** to provide multi-dimensional assessment of memory systems:

![Evaluation Modes](figures/evaluation_mode.png)

### 1. **Offline Mode**
Traditional train-test split evaluation. The agent learns from training samples (memory formation & evolution) and is tested on held-out samples (retrieval only).

**Metrics**: Average Success Rate (ASR), Average Steps (AS), F1-score, BLEU, LLM-as-Judge

### 2. **Online Mode**
Streaming evaluation where agents process samples sequentially with real-time memory updates. Performance is recorded after each sample to capture learning dynamics.

**Metrics**: Cumulative Success Rate (CSR), Learning Gain (LG), Stability Loss (SL)

### 3. **Replay Mode**
Periodic testing to measure knowledge retention and resistance to forgetting. After learning each stage, agents are tested on previously learned samples.

**Metrics**: Forgetting Rate (FR), Average Success Rate (ASR)

### 4. **Transfer Mode**
- **Cross-environment**: Tests knowledge generalization across different domains (e.g., DBâ†’OS)
- **Within-environment**: Measures forward transferâ€”how learning current samples helps future ones

**Metrics**: Transfer Gain (TG), Forward Transfer Gain (FTG)

### 5. **Repair Mode**
Tests robustness and self-correction under erroneous feedback. Agents learn under incorrect rewards, then repair memory with correct feedback.

**Metrics**: Error Robustness (ER), Repair Gain (RG), Net Recovery (NR)

## ğŸ—ï¸ Project Structure

```
AgentMemoryBench/
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â”œâ”€â”€ assignment/            # Experiment configurations
â”‚   â”‚   â””â”€â”€ default.yaml       # Main experiment config
â”‚   â”œâ”€â”€ tasks/                 # Task-specific configs (6 tasks)
â”‚   â”‚   â”œâ”€â”€ dbbench.yaml       # Database (SQL)
â”‚   â”‚   â”œâ”€â”€ os.yaml            # Operating System (Shell)
â”‚   â”‚   â”œâ”€â”€ kg.yaml            # Knowledge Graph (SPARQL)
â”‚   â”‚   â”œâ”€â”€ alfworld.yaml      # Embodied AI
â”‚   â”‚   â”œâ”€â”€ webshop.yaml       # E-commerce
â”‚   â”‚   â””â”€â”€ locomo-*.yaml      # Long conversations (0-9)
â”‚   â””â”€â”€ llmapi/                # LLM API configurations
â”‚       â”œâ”€â”€ api.yaml           # API endpoint & key for agent LLM
â”‚       â”œâ”€â”€ agent.yaml         # Agent model name
â”‚       â”œâ”€â”€ evaluate_api.yaml  # API for LoCoMo LLM-as-Judge
â”‚       â””â”€â”€ evaluate_agent.yaml# Model for evaluation
â”‚
â”œâ”€â”€ data/                       # Task datasets
â”‚   â”œâ”€â”€ dbbench/               # Database operations (SQL)
â”‚   â”œâ”€â”€ os_interaction/        # OS commands (Shell)
â”‚   â”œâ”€â”€ knowledgegraph/        # KG queries (SPARQL)
â”‚   â”œâ”€â”€ alfworld/              # Embodied tasks
â”‚   â”œâ”€â”€ webshop/               # E-commerce tasks
â”‚   â””â”€â”€ locomo/                # Long dialogues (10 conversations)
â”‚
â”œâ”€â”€ memory/                     # Memory mechanism implementations
â”‚   â”œâ”€â”€ base.py                # Base class for all memory mechanisms
â”‚   â”œâ”€â”€ registry.py            # Memory registry system
â”‚   â”œâ”€â”€ zero_shot/             # Baseline (no memory)
â”‚   â”œâ”€â”€ streamICL/             # RAG-based retrieval (topk=4)
â”‚   â”œâ”€â”€ awmPro/                # System memory via workflows (topk=8)
â”‚   â”œâ”€â”€ mem0/                  # Personal memory via preferences
â”‚   â””â”€â”€ MEMs/                  # Multi-memory coordination (proposed)
â”‚
â”œâ”€â”€ execution/                  # Execution engines
â”‚   â”œâ”€â”€ base.py                # Base execution engine
â”‚   â””â”€â”€ single_agent/          # Single-agent executor
â”‚
â”œâ”€â”€ src/                        # Core implementation
â”‚   â”œâ”€â”€ runner/                # Main entry point
â”‚   â”‚   â”œâ”€â”€ main.py            # Experiment runner
â”‚   â”‚   â”œâ”€â”€ builders.py        # Component builders
â”‚   â”‚   â”œâ”€â”€ config.py          # Configuration parser
â”‚   â”‚   â””â”€â”€ schedule_utils.py  # Scheduling utilities
â”‚   â”œâ”€â”€ client/                # Client-side scheduling
â”‚   â”‚   â”œâ”€â”€ backend.py         # Backend interface
â”‚   â”‚   â””â”€â”€ scheduler.py       # Task scheduler
â”‚   â”œâ”€â”€ server/                # Backend task servers (Docker)
â”‚   â”‚   â””â”€â”€ tasks/             # Task implementations
â”‚   â””â”€â”€ utils/                 # Analysis utilities
â”‚       â”œâ”€â”€ message_schema.py  # Message format compatibility layer
â”‚       â””â”€â”€ analyze_results_*.py # Result analysis scripts
â”‚
â”œâ”€â”€ extra/                      # Docker orchestration
â”‚   â”œâ”€â”€ docker-compose.yml     # Service definitions
â”‚   â””â”€â”€ *.Dockerfile           # Task-specific containers
â”‚
â”œâ”€â”€ outputs/                    # Experiment results
â”‚   â””â”€â”€ [timestamp]/           # Grouped by experiment time
â”‚       â””â”€â”€ [task_name]/       # Grouped by task
â”‚           â””â”€â”€ [index].json   # Individual sample results
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

<h2 id="quick-start">ğŸš€ Quick Start</h2>

### 1. Prerequisites

#### Python Environment
```bash
# Create conda environment with Python 3.9
conda create -n aMB python=3.9

# Activate environment
conda activate aMB

# Navigate to project directory
cd /path/to/AgentMemoryBench

# Install dependencies
pip install -r requirements.txt
```

#### Docker Installation
Docker is required to run backend task servers. Install Docker Desktop:
- **Windows/Mac**: [Docker Desktop](https://www.docker.com/products/docker-desktop)
- **Linux**: Follow [official guide](https://docs.docker.com/engine/install/)

### 2. Data & Model Setup

#### Knowledge Graph (Freebase) Database

The Knowledge Graph task requires the Freebase database:

1. **Download database** (~50 GB):
   - Download link: [OneDrive](https://buckeyemailosu-my.sharepoint.com/:u:/g/personal/su_809_osu_edu/Ed0SY7sAS_ZGqNTovDYhVCcBxEmZfhL3B-chAiuoZCrpVg?e=vpHUei)
   - **Recommended**: Use a download manager (e.g., Free Download Manager) instead of browser

2. **Extract** the downloaded `virtuoso_db.zip`

3. **Configure path** in `extra/docker-compose.yml` (line 114):
   ```yaml
   freebase:
     build:
       context: ..
       dockerfile: extra/freebase.Dockerfile
     volumes:
       - "/absolute/path/to/virtuoso_db:/database"  # Use absolute path
     init: true
   ```

   **Important**:
   - Use **absolute paths**
   - Windows: Use forward slashes `/` (e.g., `C:/Users/...`)
   - Example: `B:/desktop/AgentMemoryBench/virtuoso_db:/database`

#### LoCoMo Tokenizer

Download the tokenizer model for fair evaluation:

```bash
# Download xlm-roberta-base from HuggingFace
# https://huggingface.co/FacebookAI/xlm-roberta-base

# Configure path in src/server/tasks/locomo/task.py (line 47)
tokenizer = AutoTokenizer.from_pretrained("/path/to/xlm-roberta-base")
```

#### Embedding Model (for streamICL, awmPro, MEMs)

Download the embedding model for fair comparison:

```bash
# Download bge-base-en-v1.5 from HuggingFace
# https://huggingface.co/BAAI/bge-base-en-v1.5

# Configure paths in YAML files:
# - memory/streamICL/streamICL.yaml
# - memory/awmPro/awmPro.yaml
# - memory/MEMs/MEMs.yaml
```

#### Mem0 API Key

To use the Mem0 method:

1. Register for API key at [mem0.ai](https://app.mem0.ai/)
2. Configure in `memory/mem0/mem0.yaml`:
   ```yaml
   api_key: "your_mem0_api_key_here"
   wait_time: 60.0  # Recommended: 60s for system tasks, 150s for personal, 100s for mixed
   ```

### 3. Start Backend Services

```bash
# Navigate to Docker directory
cd extra

# Build required containers
docker-compose build local-os-default
docker-compose build local-os-packages
docker-compose build local-os-ubuntu
docker-compose build freebase

# Start all services
docker-compose up
```

**Note**: Keep this terminal running. Services run on `http://localhost:5038`

### 4. Configure LLM API

**Recommended**: Use [SiliconFlow API](https://siliconflow.cn/) to avoid model name mismatches.

#### Agent LLM Configuration

Edit `configs/llmapi/api.yaml`:

```yaml
base_url: "https://api.siliconflow.cn/v1"
headers:
  Content-Type: application/json
  Authorization: "Bearer YOUR_API_KEY"
```

Edit `configs/llmapi/agent.yaml`:

```yaml
model: "Qwen/Qwen2.5-14B-Instruct"  # Or your preferred model
```

#### Evaluation LLM (for LoCoMo LLM-as-Judge)

Edit `configs/llmapi/evaluate_api.yaml`:

```yaml
base_url: "https://api.siliconflow.cn/v1"
headers:
  Content-Type: application/json
  Authorization: "Bearer YOUR_API_KEY"
```

Edit `configs/llmapi/evaluate_agent.yaml`:

```yaml
model: "Qwen/Qwen2.5-14B-Instruct"  # Or evaluation model
```

### 5. Configure Experiments

Edit `configs/assignment/default.yaml`:

```yaml
# Lifelong Learning Benchmark Configuration
# é…ç½®è¦æµ‹è¯•çš„ä»»åŠ¡ã€è®°å¿†æœºåˆ¶ã€æ‰§è¡Œæ–¹æ³•å’Œå®éªŒå‚æ•°

# ===== ä»»åŠ¡é…ç½® =====
# æŒ‡å®šè¦æµ‹è¯•çš„ä»»åŠ¡åˆ—è¡¨ï¼ˆ5ä¸ªsystem memoryä»»åŠ¡+2ä¸ªuser memoryä»»åŠ¡ï¼Œå…±7ä¸ªä»»åŠ¡ï¼‰
tasks:
  # system memoryä»»åŠ¡
  # - name: dbbench-std
  #   config_path: configs/tasks/dbbench.yaml
  - name: os-std
    config_path: configs/tasks/os.yaml
  # - name: kg-std
  #   config_path: configs/tasks/kg.yaml
  # - name: alfworld-std
  #   config_path: configs/tasks/alfworld.yaml
  # - name: webshop-std
  #   config_path: configs/tasks/webshop.yaml

  # user memoryä»»åŠ¡
  # - name: locomo-0
  #   config_path: configs/tasks/locomo-0.yaml
  # - name: locomo-1
  #   config_path: configs/tasks/locomo-1.yaml
  # - name: locomo-2
  #   config_path: configs/tasks/locomo-2.yaml
  # - name: locomo-3
  #   config_path: configs/tasks/locomo-3.yaml
  # - name: locomo-4
  #   config_path: configs/tasks/locomo-4.yaml
  # - name: locomo-5
  #   config_path: configs/tasks/locomo-5.yaml
  # - name: locomo-6
  #   config_path: configs/tasks/locomo-6.yaml
  # - name: locomo-7
  #   config_path: configs/tasks/locomo-7.yaml
  # - name: locomo-8
  #   config_path: configs/tasks/locomo-8.yaml
  # - name: locomo-9
  #   config_path: configs/tasks/locomo-9.yaml

# ===== è®°å¿†æœºåˆ¶é…ç½® =====
# ä» memory æ–‡ä»¶å¤¹ä¸­é€‰æ‹©è®°å¿†æœºåˆ¶ï¼ˆç»Ÿä¸€ä½¿ç”¨ snake_case å‘½åï¼‰
memory_mechanism:
  name: zero_shot  # å¯é€‰: zero_shot, stream_icl, mem0, awm_pro

# ===== æ‰§è¡Œæ–¹æ³•é…ç½® =====
# ä» execution æ–‡ä»¶å¤¹ä¸­é€‰æ‹©æ‰§è¡Œæ–¹æ³•
execution_method:
  name: single_agent  # å½“å‰ç‰ˆæœ¬ä»…æ”¯æŒ single_agent
  config_path: execution/single_agent/single_agent.yaml

# ===== å®éªŒå‚æ•° =====
experiment:
  # è®­ç»ƒæ¨¡å¼: online (åœ¨çº¿å­¦ä¹ ) æˆ– offline (ç¦»çº¿å­¦ä¹ ) æˆ– replay (é‡æ”¾å­¦ä¹ ) æˆ– transfer (è¿ç§»å­¦ä¹ )  æˆ– repairï¼ˆä¿®å¤å­¦ä¹ ï¼‰
  training_mode: online  # online | offline | replay | transfer | repair
  
  keep_number: 700 #åªæœ‰training_modeç­‰äºonlineæ—¶ï¼Œè¿™ä¸ªå‚æ•°æ‰æœ‰æ•ˆ #ä¸ºNoneæˆ–è€…å°äºç­‰äº0ï¼Œåˆ™ä¸è¿›è¡Œæˆªæ–­

  train_size: 0.6 #åªæœ‰training_modeç­‰äºofflineæ—¶ï¼Œè¿™ä¸ªå‚æ•°æ‰æœ‰æ•ˆ

  #åœ¨transfer_taskä¸­å­¦ä¹ ï¼ˆupdate+enhanceï¼Œç›¸å½“äºonlineï¼‰ï¼Œåœ¨transfer_after_taskä¸­è¿›è¡Œæµ‹è¯•ï¼ˆä»…enhanceï¼‰
  transfer_task: dbbench-std #åªæœ‰training_modeç­‰äºtransferæ—¶ï¼Œè¿™ä¸ªå‚æ•°æ‰æœ‰æ•ˆ
  transfer_after_task: os-std #åªæœ‰training_modeç­‰äºtransferæ—¶ï¼Œè¿™ä¸ªå‚æ•°æ‰æœ‰æ•ˆ
  forward_transfer_num: 3 #åªæœ‰training_modeç­‰äºtransferä¸”transfer_task==transfer_after_taskæ—¶ï¼Œè¿™ä¸ªå‚æ•°æ‰æœ‰æ•ˆï¼Œè¡¨ç¤ºå‰å‘è¿ç§»çš„æ­¥æ•°

  #è¿™ä¸¤ä¸ªå‚æ•°çš„æ„æ€æ˜¯ï¼Œæ¯å­¦è¿‡mä¸ªæ ·æœ¬ï¼ˆupdate+enhanceï¼Œç›¸å½“äºonlineï¼‰ï¼Œå°±ä»å­¦è¿‡çš„æ‰€æœ‰æ ·æœ¬ä¸­éšæœºæŠ½æ ·nä¸ªè¿›è¡Œæµ‹è¯•ï¼ˆä»…enhanceï¼‰
  replay_m: 20 #åªæœ‰training_modeç­‰äºreplayæ—¶ï¼Œè¿™ä¸ªå‚æ•°æ‰æœ‰æ•ˆ
  replay_n: 20 #åªæœ‰training_modeç­‰äºreplayæ—¶ï¼Œè¿™ä¸ªå‚æ•°æ‰æœ‰æ•ˆ
  replay_seed: 66 #åªæœ‰training_modeç­‰äºreplayæ—¶ï¼Œè¿™ä¸ªå‚æ•°æ‰æœ‰æ•ˆ

  #è¿™ä¸¤ä¸ªå‚æ•°çš„æ„æ€æ˜¯ï¼Œå°†æ‰€æœ‰çš„caseæŒ‰ç…§måˆ†æˆxç»„ï¼Œç„¶åç»„ä¸ç»„ä¹‹å‰æ˜¯ä¸²è¡Œå­¦ä¹ çš„ï¼Œè¿™æ²¡æ¯›ç—…ï¼Œä½†æ˜¯æ¯ä¸ªç»„ä¸­ä¼šæœ‰nä¸ªcaseçš„judgeæ˜¯é”™ä¹±çš„
  repair_m: 20  # åªæœ‰training_modeç­‰äºrepairæ—¶ï¼Œè¿™ä¸ªå‚æ•°æ‰æœ‰æ•ˆï¼ˆå¯¹äºæ™®é€šä»»åŠ¡ï¼‰ï¼Œæ¯ç»„çš„æ ·æœ¬æ•°é‡
  repair_n: 20  # åªæœ‰training_modeç­‰äºrepairæ—¶ï¼Œè¿™ä¸ªå‚æ•°æ‰æœ‰æ•ˆï¼Œæ¯ç»„ä¸­éœ€è¦åè½¬å¥–åŠ±çš„æ ·æœ¬æ•°é‡
  repair_seed: 66  # åªæœ‰training_modeç­‰äºrepairæ—¶ï¼Œè¿™ä¸ªå‚æ•°æ‰æœ‰æ•ˆï¼Œé€‰æ‹©åè½¬æ ·æœ¬çš„éšæœºç§å­
  repair_size_locomo: 0.5  # åªæœ‰training_modeç­‰äºrepairä¸”ä»»åŠ¡ä¸ºlocomoæ—¶æœ‰æ•ˆï¼Œè¡¨ç¤ºæ¯ä¸ªsessionä¸­éœ€è¦åè½¬çš„QAæ¯”ä¾‹ï¼ˆ0-1ä¹‹é—´ï¼‰

  ...
  
  cross_task: False  # True | False

  # æ•°æ®æ‰“ä¹±: æ˜¯å¦æ‰“ä¹±ä»»åŠ¡é¡ºåºï¼Œå¯ä»¥è®¾ç½®éšæœºç§å­
  shuffle:
    enabled: True  # True | False
    seed: 66  # æ•´æ•°ï¼Œå¦‚æœ enabled ä¸º true æ—¶ä½¿ç”¨
```

### 6. Run Experiments

```bash
# Run with default configuration
python -m src.runner.main

# Or specify custom config
python -m src.runner.main --config configs/assignment/my_experiment.yaml
```

<h2 id="creating-custom-memory-mechanisms">ğŸ› ï¸ Creating Custom Memory Mechanisms</h2>

### Step 1: Implement Memory Class

Create a new directory under `memory/` (e.g., `memory/my_memory/`):

```python
# memory/my_memory/my_memory.py
from __future__ import annotations
from typing import List, Dict, Any
import yaml
from ..base import MemoryMechanism

class MyMemory(MemoryMechanism):
    """Your custom memory mechanism"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        # Initialize your memory storage

    def use_memory(
        self,
        task: str,
        messages: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Enhance messages with memory before LLM call.

        Args:
            task: Task name (e.g., "dbbench-std", "os-std")
            messages: Original message list

        Returns:
            Enhanced messages with retrieved memory
        """
        # Retrieve relevant experience from memory
        # Inject experience into messages
        return messages  # Return enhanced messages

    def update_memory(
        self,
        task: str,
        history: List[Dict[str, Any]],
        result: Dict[str, Any]
    ) -> None:
        """
        Update memory after sample execution.

        Args:
            task: Task name
            history: Full dialogue history
            result: Execution result (reward, status, etc.)
        """
        # Update your memory storage based on history and result
        pass

def load_my_memory_from_yaml(config_path: str) -> MyMemory:
    """Load memory from YAML config"""
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f) or {}
    return MyMemory(config)
```

Create configuration file `memory/my_memory/my_memory.yaml`:

```yaml
name: my_memory
description: "My custom memory mechanism"

# Your configuration parameters
param1: value1
param2: value2
```

### Step 2: Register in Registry

Add registration in `memory/registry.py`:

```python
def _register_all_memories():
    # ... existing registrations ...

    # Register your memory mechanism (use snake_case)
    from memory.my_memory.my_memory import load_my_memory_from_yaml
    register_memory(
        name="my_memory",  # Use snake_case
        loader_func=load_my_memory_from_yaml,
        default_config_path="memory/my_memory/my_memory.yaml",
    )
```

### Step 3: Use Your Memory

Configure in `configs/assignment/default.yaml`:

```yaml
memory_mechanism:
  name: my_memory  # Use snake_case naming
  config_path: memory/my_memory/my_memory.yaml  # Optional
```

<h2 id="implemented-memory-mechanisms">ğŸ“ˆ Implemented Memory Mechanisms</h2>

| Method | Type | Description | Key Features |
|--------|------|-------------|--------------|
| **zero_shot** | Baseline | No memory | Reflects base LLM capability |
| **streamICL** | Retrieval | RAG-based ICL | Stores full trajectories, topk=4 |
| **awmPro** | System | Workflow memory | Extracts execution patterns, topk=8 |
| **mem0** | Personal | Preference memory | Graph-based storage with ADD/UPDATE/DELETE |
| **MEMs** | Hybrid | Multi-memory | Coordinates system & personal memory via trigger model |

### Fair Comparison Notes

- **streamICL**: Uses topk=4 following [original paper](https://arxiv.org/abs/2406.08747)
- **awmPro**: Modified from [AWM](https://arxiv.org/abs/2409.07429) with mem0-inspired management, topk=8 based on workflow induction experiments
- **mem0**: Uses best practices from [official implementation](https://arxiv.org/abs/2504.19413)

See ablation studies in paper for detailed topk analysis across different tasks.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Task datasets adapted from AgentBench and LoCoMo
- Evaluation protocols inspired by continual learning literature
- Memory baselines from StreamBench, AWM, and Mem0
